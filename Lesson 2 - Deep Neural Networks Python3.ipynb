{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Neural Networks\n",
    "\n",
    "Previously in Lesson 1, we created a pickle with formatted datasets for training, development and testing on the [notMNIST dataset](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html).\n",
    "\n",
    "The goal of this assignment is to progressively train deeper and more accurate models using TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data from Lesson 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples in train dataset: 197.901 thousand\n",
      "Dimensions of train labels (197901,)\n",
      "Dimensions of train dataset (197901, 28, 28)\n",
      "\n",
      "Examples in valid dataset: 9.876 thousand\n",
      "Dimensions of valid labels (9876,)\n",
      "Dimensions of valid dataset (9876, 28, 28)\n",
      "\n",
      "Examples in test dataset: 9.857 thousand\n",
      "Dimensions of test labels (9857,)\n",
      "Dimensions of test dataset (9857, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'data/notMNIST_clean_python3.pickle'\n",
    "\n",
    "dic_file = pickle.load(open(pickle_file, 'rb'))\n",
    "train_dataset = dic_file[\"train_dataset\"]\n",
    "train_labels = dic_file[\"train_labels\"]\n",
    "valid_dataset = dic_file[\"valid_dataset\"]\n",
    "valid_labels = dic_file[\"valid_labels\"]\n",
    "test_dataset = dic_file[\"test_dataset\"]\n",
    "test_labels = dic_file[\"test_labels\"]\n",
    "\n",
    "del dic_file # freee up memory\n",
    "\n",
    "print(\"Examples in train dataset:\", len(train_labels) / float(1000), \"thousand\")\n",
    "print(\"Dimensions of train labels\", train_labels.shape)\n",
    "print(\"Dimensions of train dataset\", train_dataset.shape)\n",
    "print(\"\\nExamples in valid dataset:\", len(valid_labels) / float(1000), \"thousand\")\n",
    "print(\"Dimensions of valid labels\", valid_labels.shape)\n",
    "print(\"Dimensions of valid dataset\", valid_dataset.shape)\n",
    "print(\"\\nExamples in test dataset:\", len(test_labels) / float(1000), \"thousand\")\n",
    "print(\"Dimensions of test labels\", test_labels.shape)\n",
    "print(\"Dimensions of test dataset\", test_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAEKCAYAAAAy4ujqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc4e8d7a358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# view images\n",
    "def print_images(image, label):\n",
    "    plt.imshow(image, cmap = \"gray\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(label)\n",
    "    plt.show()\n",
    "    \n",
    "index = np.random.randint(0, 1000)\n",
    "print_images(test_dataset[index], test_labels[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape Data intro accordning matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shapes --> Dataset: (197901, 784)   Labels: (197901, 10)\n",
      "Valid Shapes --> Dataset: (9876, 784)   Labels: (9876, 10)\n",
      "Test Shapes  --> Dataset: (9857, 784)   Labels: (9857, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    # as.type is not needed as the array is already float32 but just in case\n",
    "    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "    # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "    labels = (np.arange(num_labels) == labels[:, None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "\n",
    "print(\"Train Shapes --> Dataset: %s   Labels: %s\" %(train_dataset.shape, train_labels.shape))\n",
    "print(\"Valid Shapes --> Dataset: %s   Labels: %s\" %(valid_dataset.shape, valid_labels.shape))\n",
    "print(\"Test Shapes  --> Dataset: %s   Labels: %s\" %(test_dataset.shape, test_labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAEKCAYAAAAy4ujqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRCJ+QkILkYifkNBCJOInJLQQ/x9Z2NtQbex0rgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc4e8da54e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# images\n",
    "def print_images(image, label):\n",
    "    plt.imshow(image.reshape(image_size, image_size), cmap = \"gray\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(label)\n",
    "    plt.show()\n",
    "    \n",
    "index = np.random.randint(0, 1000)\n",
    "print_images(train_dataset[index], train_labels[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Logistic Regression - Gradient Descent\n",
    "We're first going to train a multinomial logistic regression using simple gradient descent.\n",
    "\n",
    "TensorFlow works like this:\n",
    "* First you describe the computation that you want to see performed: what the inputs, the variables, and the operations look like. These get created as nodes over a computation graph. This description is all contained within the block below:\n",
    "\n",
    "      with graph.as_default():\n",
    "          ...\n",
    "\n",
    "* Then you can run the operations on this graph as many times as you want by calling `session.run()`, providing it outputs to fetch from the graph that get returned. This runtime operation is all contained in the block below:\n",
    "\n",
    "      with tf.Session(graph=graph) as session:\n",
    "          ...\n",
    "\n",
    "Let's load all the data into TensorFlow and build the computation graph corresponding to our training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# With gradient descent training, even this much data is prohibitive.\n",
    "# Subset the training data for faster turnaround.\n",
    "train_subset_size = 10000\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    \"\"\"Constants - Input Data\n",
    "    \n",
    "    Load the training, validation and test data into constant that \n",
    "    attached to the graph\n",
    "    \"\"\"\n",
    "    tf_train_dataset = tf.constant(train_dataset[:train_subset_size])\n",
    "    tf_train_labels = tf.constant(train_labels[:train_subset_size])\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    \"\"\"    Variables - Weights\n",
    "    \n",
    "    These are the parameters that we are going to be training.\n",
    "    Weights are initiallized randomly and the biases to zero\n",
    "    \"\"\"\n",
    "    weights = tf.Variable(tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    \"\"\"Training Computation\n",
    "  \n",
    "    We multiply the inputs with the weight matrix, and add biases. \n",
    "    We compute then calculate the softmax and cross-entropy (it's one \n",
    "    operation in TensorFlow, because it's very common, and it can be optimized). \n",
    "    We take the average of this cross-entropy across all training examples: that's our loss.\n",
    "    \"\"\"\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "    \"\"\"Optimizer\n",
    "    We now find the minimum of this loss using gradient descent\n",
    "    \"\"\"\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    \"\"\"Predictions for the training, validation and test data.\n",
    "    \n",
    "    These are not part of training, but merely here so that we can report\n",
    "    accuracy figures as we train\n",
    "    \"\"\"\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 18.558634\n",
      "Training Accuracy: \t10.3%\n",
      "Validation Accuracy: \t11.9%\n",
      "Loss at step 100: 2.343371\n",
      "Training Accuracy: \t71.9%\n",
      "Validation Accuracy: \t70.6%\n",
      "Loss at step 200: 1.872649\n",
      "Training Accuracy: \t74.8%\n",
      "Validation Accuracy: \t72.9%\n",
      "Loss at step 300: 1.623941\n",
      "Training Accuracy: \t76.0%\n",
      "Validation Accuracy: \t73.6%\n",
      "Loss at step 400: 1.455439\n",
      "Training Accuracy: \t76.7%\n",
      "Validation Accuracy: \t74.2%\n",
      "Loss at step 500: 1.330029\n",
      "Training Accuracy: \t77.4%\n",
      "Validation Accuracy: \t74.6%\n",
      "Loss at step 600: 1.231195\n",
      "Training Accuracy: \t78.0%\n",
      "Validation Accuracy: \t74.9%\n",
      "Loss at step 700: 1.150552\n",
      "Training Accuracy: \t78.5%\n",
      "Validation Accuracy: \t75.0%\n",
      "Loss at step 800: 1.083279\n",
      "Training Accuracy: \t79.0%\n",
      "Validation Accuracy: \t75.2%\n",
      "\n",
      "Test Accuracy: \t\t82.1%\n",
      "CPU times: user 2min 21s, sys: 16 s, total: 2min 37s\n",
      "Wall time: 52.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Running the computation\n",
    "num_steps = 801\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    return 100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0]\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    \"\"\"  This is a one-time operation which ensures the parameters get initialized as\n",
    "    we described in the graph: random weights for the matrix, zeros for the\n",
    "    biases.\"\"\"\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        \"\"\"Run the computations. We tell .run() that we want to run the optimizer,\n",
    "        and get the loss value and the training predictions returned as numpy\n",
    "        arrays.\"\"\"\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "        if step % 100 == 0:\n",
    "            print(\"Loss at step %d: %f\" %(step, l))\n",
    "            print(\"Training Accuracy: \\t%.1f%%\" % accuracy(predictions, train_labels[:train_subset_size]))\n",
    "            \"\"\"Calling .eval() on valid_prediction is basically like calling run(), but\n",
    "            just to get that one numpy array. Note that it recomputes all its graph\n",
    "            dependencies.\"\"\"\n",
    "            print(\"Validation Accuracy: \\t%.1f%%\" %accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"\\nTest Accuracy: \\t\\t%.1f%%\" %accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent\n",
    "The graph will be similar, except that instead of holding all the training data into a constant node, we create a `Placeholder` node which will be fed actual data at every call of `session.run()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \"\"\"Input data. For the training data, we use a placeholder that will be fed\n",
    "    at run time with a training minibatch\"\"\"\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape = (batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape = (batch_size, num_labels))\n",
    "    \n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    \"\"\"    Variables - Weights\n",
    "    \n",
    "    These are the parameters that we are going to be training.\n",
    "    Weights are initiallized randomly and the biases to zero\n",
    "    \"\"\"\n",
    "    weights = tf.Variable(tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    \"\"\"Training Computation\n",
    "  \n",
    "    We multiply the inputs with the weight matrix, and add biases. \n",
    "    We compute then calculate the softmax and cross-entropy (it's one \n",
    "    operation in TensorFlow, because it's very common, and it can be optimized). \n",
    "    We take the average of this cross-entropy across all training examples: that's our loss.\n",
    "    \"\"\"\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "    \"\"\"Optimizer\n",
    "    We now find the minimum of this loss using gradient descent\n",
    "    \"\"\"\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    \"\"\"Predictions for the training, validation and test data.\n",
    "    \n",
    "    These are not part of training, but merely here so that we can report\n",
    "    accuracy figures as we train\n",
    "    \"\"\"\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 17.499760\n",
      "Minibatch accuracy: \t13.3%\n",
      "Validation accuracy: \t12.0%\n",
      "Minibatch loss at step 500: 1.993294\n",
      "Minibatch accuracy: \t68.0%\n",
      "Validation accuracy: \t75.0%\n",
      "Minibatch loss at step 1000: 1.342129\n",
      "Minibatch accuracy: \t79.7%\n",
      "Validation accuracy: \t76.3%\n",
      "Minibatch loss at step 1500: 0.968957\n",
      "Minibatch accuracy: \t81.2%\n",
      "Validation accuracy: \t76.7%\n",
      "Minibatch loss at step 2000: 1.997852\n",
      "Minibatch accuracy: \t67.2%\n",
      "Validation accuracy: \t77.7%\n",
      "Minibatch loss at step 2500: 1.030935\n",
      "Minibatch accuracy: \t80.5%\n",
      "Validation accuracy: \t77.8%\n",
      "Minibatch loss at step 3000: 0.841187\n",
      "Minibatch accuracy: \t81.2%\n",
      "Validation accuracy: \t78.2%\n",
      "\n",
      "Test accuracy: 85.2%\n",
      "CPU times: user 7.84 s, sys: 2.14 s, total: 9.98 s\n",
      "Wall time: 4.97 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        \n",
    "        \"\"\"This offset is an integer of the batch size, Such that \n",
    "        we are taking the batches not in the order they are presented.\"\"\"\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        \n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size)]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size)]\n",
    "        \n",
    "        \"\"\"Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        and the value is the numpy array to feed to it.\"\"\"\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        \n",
    "        # Perform calculations\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict = feed_dict)\n",
    "        \n",
    "        # Give feedback every 500 steps\n",
    "        if step % 500 == 0:\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: \\t%.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: \\t%.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"\\nTest accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent + Hidden Layer\n",
    "Turn the logistic regression example with SGD into a 1-hidden layer neural network with rectified linear units [nn.relu()](https://www.tensorflow.org/versions/r0.7/api_docs/python/nn.html#relu) and 1024 hidden nodes. This model should improve your validation / test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \"\"\"Input data. For the training data, we use a placeholder that will be fed\n",
    "    at run time with a training minibatch\"\"\"\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape = (batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape = (batch_size, num_labels))\n",
    "    \n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    \"\"\"    Variables - Weights\n",
    "    \n",
    "    These are the parameters that we are going to be training.\n",
    "    Weights are initiallized randomly and the biases to zero\n",
    "    \"\"\"\n",
    "    weights = tf.Variable(tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    \"\"\"Training Computation\n",
    "  \n",
    "    We multiply the inputs with the weight matrix, and add biases. \n",
    "    We compute then calculate the softmax and cross-entropy (it's one \n",
    "    operation in TensorFlow, because it's very common, and it can be optimized). \n",
    "    We take the average of this cross-entropy across all training examples: that's our loss.\n",
    "    \"\"\"\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "    \"\"\"Optimizer\n",
    "    We now find the minimum of this loss using gradient descent\n",
    "    \"\"\"\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    \"\"\"Predictions for the training, validation and test data.\n",
    "    \n",
    "    These are not part of training, but merely here so that we can report\n",
    "    accuracy figures as we train\n",
    "    \"\"\"\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 16.924141\n",
      "Minibatch accuracy: \t10.9%\n",
      "Validation accuracy: \t12.9%\n",
      "Minibatch loss at step 500: 1.968161\n",
      "Minibatch accuracy: \t74.2%\n",
      "Validation accuracy: \t73.9%\n",
      "Minibatch loss at step 1000: 1.413652\n",
      "Minibatch accuracy: \t76.6%\n",
      "Validation accuracy: \t76.1%\n",
      "Minibatch loss at step 1500: 1.069954\n",
      "Minibatch accuracy: \t78.1%\n",
      "Validation accuracy: \t76.7%\n",
      "Minibatch loss at step 2000: 1.822005\n",
      "Minibatch accuracy: \t69.5%\n",
      "Validation accuracy: \t77.4%\n",
      "Minibatch loss at step 2500: 0.991371\n",
      "Minibatch accuracy: \t78.9%\n",
      "Validation accuracy: \t78.0%\n",
      "Minibatch loss at step 3000: 0.712138\n",
      "Minibatch accuracy: \t84.4%\n",
      "Validation accuracy: \t77.9%\n",
      "\n",
      "Test accuracy: 85.2%\n",
      "CPU times: user 7.98 s, sys: 2.19 s, total: 10.2 s\n",
      "Wall time: 5.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        \n",
    "        \"\"\"This offset is an integer of the batch size, Such that \n",
    "        we are taking the batches not in the order they are presented.\"\"\"\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        \n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size)]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size)]\n",
    "        \n",
    "        \"\"\"Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        and the value is the numpy array to feed to it.\"\"\"\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        \n",
    "        # Perform calculations\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict = feed_dict)\n",
    "        \n",
    "        # Give feedback every 500 steps\n",
    "        if step % 500 == 0:\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: \\t%.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: \\t%.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"\\nTest accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "hidden_layer_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \"\"\"Input data. For the training data, we use a placeholder that will be fed\n",
    "    at run time with a training minibatch\"\"\"\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape = (batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape = (batch_size, num_labels))\n",
    "    \n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    \"\"\"    Variables - Weights\n",
    "    \n",
    "    These are the parameters that we are going to be training.\n",
    "    Weights are initiallized randomly and the biases to zero\n",
    "    \"\"\"\n",
    "    weights1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_layer_nodes]))\n",
    "    biases1 = tf.Variable(tf.zeros([hidden_layer_nodes]))\n",
    "    \n",
    "    weights2 = tf.Variable(tf.truncated_normal([hidden_layer_nodes, num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    \"\"\"Training Computation\n",
    "  \n",
    "    We multiply the inputs with the weight matrix, and add biases. \n",
    "    We compute then calculate the softmax and cross-entropy (it's one \n",
    "    operation in TensorFlow, because it's very common, and it can be optimized). \n",
    "    We take the average of this cross-entropy across all training examples: that's our loss.\n",
    "    \"\"\"\n",
    "    hidden = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    logits = tf.matmul(hidden, weights2) + biases2\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "    \"\"\"Optimizer\n",
    "    We now find the minimum of this loss using gradient descent\n",
    "    \"\"\"\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    \"\"\"Predictions for the training, validation and test data.\n",
    "    \n",
    "    These are not part of training, but merely here so that we can report\n",
    "    accuracy figures as we train\n",
    "    \"\"\"\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    hidden_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(hidden_valid, weights2) + biases2)\n",
    "    \n",
    "    hidden_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(hidden_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 390.293579\n",
      "Minibatch accuracy: \t11.7%\n",
      "Validation accuracy: \t24.7%\n",
      "Minibatch loss at step 500: 21.466150\n",
      "Minibatch accuracy: \t77.3%\n",
      "Validation accuracy: \t80.7%\n",
      "Minibatch loss at step 1000: 13.902340\n",
      "Minibatch accuracy: \t87.5%\n",
      "Validation accuracy: \t80.5%\n",
      "Minibatch loss at step 1500: 7.822052\n",
      "Minibatch accuracy: \t87.5%\n",
      "Validation accuracy: \t81.5%\n",
      "Minibatch loss at step 2000: 5.972208\n",
      "Minibatch accuracy: \t79.7%\n",
      "Validation accuracy: \t82.5%\n",
      "Minibatch loss at step 2500: 1.607383\n",
      "Minibatch accuracy: \t82.8%\n",
      "Validation accuracy: \t81.4%\n",
      "Minibatch loss at step 3000: 1.943299\n",
      "Minibatch accuracy: \t88.3%\n",
      "Validation accuracy: \t81.7%\n",
      "\n",
      "Test accuracy: 88.2%\n",
      "CPU times: user 3min 52s, sys: 10.6 s, total: 4min 3s\n",
      "Wall time: 1min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        \n",
    "        \"\"\"This offset is an integer of the batch size, Such that \n",
    "        we are taking the batches not in the order they are presented.\"\"\"\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        \n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size)]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size)]\n",
    "        \n",
    "        \"\"\"Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        and the value is the numpy array to feed to it.\"\"\"\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        \n",
    "        # Perform calculations\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict = feed_dict)\n",
    "        \n",
    "        # Give feedback every 500 steps\n",
    "        if step % 500 == 0:\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: \\t%.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: \\t%.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"\\nTest accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
