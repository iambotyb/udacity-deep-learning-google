{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal is to explore regularization techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from six.moves import cPickle as pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (197901, 28, 28) (197901,)\n",
      "Validation set (9876, 28, 28) (9876,)\n",
      "Test set (9857, 28, 28) (9857,)\n"
     ]
    }
   ],
   "source": [
    "# get data\n",
    "pickle_file = 'data/notMNIST_clean_python3.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape Data such that each example is a row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shapes --> Dataset: (197901, 784)   Labels: (197901, 10)\n",
      "Valid Shapes --> Dataset: (9876, 784)   Labels: (9876, 10)\n",
      "Test Shapes  --> Dataset: (9857, 784)   Labels: (9857, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    # as.type is not needed as the array is already float32 but just in case\n",
    "    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "    # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "    labels = (np.arange(num_labels) == labels[:, None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "\n",
    "print(\"Train Shapes --> Dataset: %s   Labels: %s\" %(train_dataset.shape, train_labels.shape))\n",
    "print(\"Valid Shapes --> Dataset: %s   Labels: %s\" %(valid_dataset.shape, valid_labels.shape))\n",
    "print(\"Test Shapes  --> Dataset: %s   Labels: %s\" %(test_dataset.shape, test_labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return 100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Model + SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    \"\"\"Input data. For the training data, we use a placeholder that will be fed\n",
    "    at run time with a training minibatch\"\"\"\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape = (batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape = (batch_size, num_labels))\n",
    "    \n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    beta = tf.placeholder(tf.float32)\n",
    "    \n",
    "    \"\"\"    Variables - Weights\n",
    "    \n",
    "    These are the parameters that we are going to be training.\n",
    "    Weights are initiallized randomly and the biases to zero\n",
    "    \"\"\"\n",
    "    weights = tf.Variable(tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    \"\"\"Training Computation\n",
    "  \n",
    "    We multiply the inputs with the weight matrix, and add biases. \n",
    "    We compute then calculate the softmax and cross-entropy (it's one \n",
    "    operation in TensorFlow, because it's very common, and it can be optimized). \n",
    "    We take the average of this cross-entropy across all training examples: that's our loss.\n",
    "    \"\"\"\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + \\\n",
    "           beta * tf.nn.l2_loss(weights)\n",
    "    \n",
    "    \"\"\"Optimizer\n",
    "    We now find the minimum of this loss using gradient descent\n",
    "    \"\"\"\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    \"\"\"Predictions for the training, validation and test data.\n",
    "    \n",
    "    These are not part of training, but merely here so that we can report\n",
    "    accuracy figures as we train\n",
    "    \"\"\"\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization Parameter: 0.000\tTest accuracy: 85.6%\n",
      "Regularization Parameter: 0.000\tTest accuracy: 85.6%\n",
      "Regularization Parameter: 0.000\tTest accuracy: 85.9%\n",
      "Regularization Parameter: 0.000\tTest accuracy: 86.3%\n",
      "Regularization Parameter: 0.000\tTest accuracy: 87.3%\n",
      "Regularization Parameter: 0.001\tTest accuracy: 88.7%\n",
      "Regularization Parameter: 0.003\tTest accuracy: 88.6%\n",
      "Regularization Parameter: 0.010\tTest accuracy: 87.9%\n",
      "CPU times: user 1min 2s, sys: 20.4 s, total: 1min 22s\n",
      "Wall time: 40.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_steps = 3001\n",
    "regularization_parameters = np.array([0, .00001, .00003, .0001, .0003, .001, .003, .01]).astype(np.float32)\n",
    "acc = []\n",
    "\n",
    "for reg_param in regularization_parameters:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.initialize_all_variables().run()\n",
    "        for step in range(num_steps):\n",
    "\n",
    "            \"\"\"This offset is an integer of the batch size, Such that \n",
    "            we are taking the batches not in the order they are presented.\"\"\"\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + batch_size)]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size)]\n",
    "\n",
    "            \"\"\"Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            and the value is the numpy array to feed to it.\"\"\"\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta : reg_param}\n",
    "\n",
    "            # Perform calculations\n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict = feed_dict)\n",
    "\n",
    "#             # Give feedback every 500 steps\n",
    "#             if step % 500 == 0:\n",
    "#                 print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "#                 print(\"Minibatch accuracy: \\t%.1f%%\" % accuracy(predictions, batch_labels))\n",
    "#                 print(\"Validation accuracy: \\t%.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "        acc.append(accuracy(test_prediction.eval(), test_labels))\n",
    "        print(\"Regularization Parameter: %.3f\\tTest accuracy: %.1f%%\" %(reg_param, acc[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc83413a080>]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEACAYAAABcXmojAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4lAwM7MSh4KZmZU4FMzMrOT/A/eqf64eocyeAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc823fa9e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(regularization_parameters, acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Model + SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "hidden_layer_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \"\"\"Input data. For the training data, we use a placeholder that will be fed\n",
    "    at run time with a training minibatch\"\"\"\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape = (batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape = (batch_size, num_labels))\n",
    "    \n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    beta = tf.placeholder(tf.float32)\n",
    "    \n",
    "    \"\"\"    Variables - Weights\n",
    "    \n",
    "    These are the parameters that we are going to be training.\n",
    "    Weights are initiallized randomly and the biases to zero\n",
    "    \"\"\"\n",
    "    weights1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_layer_nodes]))\n",
    "    biases1 = tf.Variable(tf.zeros([hidden_layer_nodes]))\n",
    "    \n",
    "    weights2 = tf.Variable(tf.truncated_normal([hidden_layer_nodes, num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    \"\"\"Training Computation\n",
    "  \n",
    "    We multiply the inputs with the weight matrix, and add biases. \n",
    "    We compute then calculate the softmax and cross-entropy (it's one \n",
    "    operation in TensorFlow, because it's very common, and it can be optimized). \n",
    "    We take the average of this cross-entropy across all training examples: that's our loss.\n",
    "    \"\"\"\n",
    "    hidden = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    logits = tf.matmul(hidden, weights2) + biases2\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))+ \\\n",
    "           beta * tf.nn.l2_loss(weights1) + beta * tf.nn.l2_loss(weights2)\n",
    "    \n",
    "    \"\"\"Optimizer\n",
    "    We now find the minimum of this loss using gradient descent\n",
    "    \"\"\"\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    \"\"\"Predictions for the training, validation and test data.\n",
    "    \n",
    "    These are not part of training, but merely here so that we can report\n",
    "    accuracy figures as we train\n",
    "    \"\"\"\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    hidden_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(hidden_valid, weights2) + biases2)\n",
    "    \n",
    "    hidden_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(hidden_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization Parameter: 0.000\tTest accuracy: 88.9%\n",
      "Regularization Parameter: 0.000\tTest accuracy: 88.7%\n",
      "Regularization Parameter: 0.000\tTest accuracy: 88.4%\n",
      "Regularization Parameter: 0.000\tTest accuracy: 88.5%\n",
      "Regularization Parameter: 0.000\tTest accuracy: 89.2%\n",
      "Regularization Parameter: 0.001\tTest accuracy: 92.9%\n",
      "Regularization Parameter: 0.003\tTest accuracy: 92.5%\n",
      "Regularization Parameter: 0.010\tTest accuracy: 89.9%\n",
      "CPU times: user 29min 45s, sys: 2min 19s, total: 32min 5s\n",
      "Wall time: 11min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_steps = 3001\n",
    "regularization_parameters = np.array([0, .00001, .00003, .0001, .0003, .001, .003, .01]).astype(np.float32)\n",
    "acc = []\n",
    "\n",
    "for reg_param in regularization_parameters:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.initialize_all_variables().run()\n",
    "        for step in range(num_steps):\n",
    "\n",
    "            \"\"\"This offset is an integer of the batch size, Such that \n",
    "            we are taking the batches not in the order they are presented.\"\"\"\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + batch_size)]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size)]\n",
    "\n",
    "            \"\"\"Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            and the value is the numpy array to feed to it.\"\"\"\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta : reg_param}\n",
    "\n",
    "            # Perform calculations\n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict = feed_dict)\n",
    "\n",
    "#             # Give feedback every 500 steps\n",
    "#             if step % 500 == 0:\n",
    "#                 print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "#                 print(\"Minibatch accuracy: \\t%.1f%%\" % accuracy(predictions, batch_labels))\n",
    "#                 print(\"Validation accuracy: \\t%.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "        acc.append(accuracy(test_prediction.eval(), test_labels))\n",
    "        print(\"Regularization Parameter: %.3f\\tTest accuracy: %.1f%%\" %(reg_param, acc[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc823d3ffd0>]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEACAYAAACwB81wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAHntJREFUeJzt3XeYVOXZx/HvTZNmAaW9FBVsiFIlYEHXoElUBEts0Vii\nMUZf9VJjQSyYiBKNSYxGjcbYYomvXVQEDCtFilFAQIRlFwURUUQBUfrz/nEPLkFgZ3bnzJmZ8/tc\n1167Oztnzz2H5bdnn+c597EQAiIiUvxqxV2AiIjkhgJfRCQhFPgiIgmhwBcRSQgFvohIQijwRUQS\nIq3AN7NLzWx66u2S1GO/NbNpZjbVzEaZWZtoSxURkZqwqtbhm1kn4EmgJ7AOeA24APgshPB16jkX\nA11CCOdFW66IiFRXOmf4HYFJIYTVIYT1wBjghI1hn9IIWBJFgSIikh110njODOBmM2sCrAaOBt4G\nMLObgTOBb4BeURUpIiI1V+WQDoCZnQNcBHwNzARWhxAu3+TrVwP7hBDOiapQERGpmbQC/782MBsC\nLAgh3LfJY22BV0MI+2/h+WrWIyJSDSEEy+b3S3eVTrPU+3bA8cATZrbHJk85Dpi6te1DCHoLgRtv\nvDH2GvLlTcdCx0LHYttvUUhnDB/gWTNrCqwFLgwhLDezf5jZXsB6oAL4dSQViohIVqQV+CGEQ7fw\n2E+zX072hQCvvw7du0Pz5nFXIyISn3TP8AvWhx/C8cfDdttBx47Qrx8ceyzsvz9YVkfHqlZSUpLb\nHeYxHYtKOhaVdCyilfGkbcY7MAtR72NbRo6EoUPh1VdhzBgYNgxefhnWrfPw79cPDj8cGjSIrUQR\nke8xM0Ick7aFbO5c2GMPP8M/8ki4804oL/dhnt12818GLVrAgAHwwAPwySdxVywiEo2iP8O/4gpo\n2RKuvHLrz1m6FIYP97P/4cNh990rh366d4daRf9rUUTyTRRn+EUf+AMGwNln+zh+Otauhbfe8vAf\nNgy++gqOOcZ/ARxxBDRuHGm5IiKAAr9aOnWCp57ySdrqmDu3MvwnTYJDDqkc+9911+zWKiKykQI/\nQxs2QKNGsGSJv6+pZct8EnjYMJ8EbtGicuinVy+oXbvm+xARAQV+xhYsgN69YeHC7H/v9evh7bd9\nxc+wYT7Ze9RR/gvgxz+GHXfM/j5FJDkU+BkaPRoGD4Y334x+X/PnwyuvePiPHQs9e1YO/ey5Z/T7\nF5HiosDP0AMPwMSJ8OCDud3vypXwxhuVY//bb1859HPwwVC3bm7rEZHCo3X4Gdq4Bj/XGjWC/v3h\n/vvh44/hySdhhx18aWiLFnDqqfDPf8IXX+S+NhFJrqI+wz/xRA/Xk06KZfdbtGiRT/gOGwb//jd0\n7lw59LPvvrlv9yAi+UlDOhnq0gUefhi6dYtl91VatQpKSyvbPdSuXRn+hx3mVweLSDIp8DMQgo+d\nf/KJD6fkuxBg5szK8J8xA/r29fA/+mi/WlhEkkOBn4FFi6BrV1i8OOe7zoolS+C11/wXwIgRvtLn\n2GP9F0DXrhr6ESl2CvwMjB0L11wD48fnfNdZt3YtjBvnZ/4vvwzfflvZ7qFvX2jYMO4KRSTbtEon\nA3Gt0IlC3brewvmPf4SyMp/s3Xtv+NOffKinXz+47z6/0ExEZGuK9gx/0CCoXx+uvz7nu86pr77y\nVs/DhvkQUNu2lRO/PXuq06dIodIZfgaK6Qx/W3baCU45BR57zOcr7r7bb+5y7rnQqhWccw489xys\nWBF3pSISt6I9w+/Rw4c5evbM+a7zxrx5le0e3nrL+woddxycdVZ2msmJSHQ0aZumEPzM98MPoUmT\nnO46b339NYwa5Vf4jh0Ll1wCF13kx0lE8o+GdNK0ZAnUqaOw31Tjxn52/8wzfrHXnDk+5DVoEHz+\nedzViUguFGXgJ2X8vro6doRHHvH2zkuX+oqfyy/X/XxFip0CP8F23x3uvRemT/fP99sPLrjAx/5F\npPgo8IXWrX2N/+zZsMsuPtF95pkwa1bclYlINinw5TvNmsHNN/vx23tvKCmBn/4UpkyJuzIRyQYF\nvnzPTjv5ZG5Fhd+wpV8/b+Xw1ltxVyYiNaHAl61q1Aguu8yDv39/OP10b/EwapQvfRWRwlJ06/CX\nLoXddoNly9RRMtvWrvW7d916q7ecvu46P/vXcRbJPq3DT0N5uZ/dK4Syr25dn8ydMcNv13jDDd6q\n+V//gvXr465ORKpSdIGv4Zzo1a7tk7nvvutn+3fe6bdnfOgh/ytARPKTAl+qzczvxjV+vPctevxx\nv1HLPff47RtFJL8o8KXGzConc596CoYPh/bt4Q9/8B4+IpIfFPiSVb17w0sveW/+t9/24P/tb+HL\nL+OuTEQU+BKJLl18MnfsWG/VsMceMHAgfPZZ3JWJJFdRBf7y5T6E0KpV3JXIRnvv7ZO5777r/z77\n7AOXXgoffxx3ZSLJk1bgm9mlZjY99XZJ6rHbzGyWmU01s2fNbIdoS61aeTl06KAlmflo113hr3+F\nmTOhXj3o3BnOP9//zUQkN6oMfDPrBJwLHAB0BfqZWXtgBNAphNAVKAMGRlloOjSck/9atYLbb/eb\nsbdsCb16wRln+C8CEYlWOmf4HYFJIYTVIYT1wBjghBDCqBDChtRzJgJtoioyXQr8wrHzzj6ZW1Hh\nbZn79oUTToB33om7MpHilU7gzwD6mFkTM2sIHA203ew5vwBey3ZxmVLgF54ddoBrrvHgP+wwvyvX\nUUfBuHFxVyZSfOpU9YQQwgdm9ntgJPA1MAX47kJ6MxsErA0hPLG17zF48ODvPi4pKaGkpKT6FW/D\n3Lne4EsKT8OGPpl7wQXw6KN+o/U2bbxr55FHal5Gil9paSmlpaWR7iPj5mlmNgRYEEK4z8zOBn4J\n/DCEsHorz89Z87TWrWHCBGjXLie7kwitW+fLOm+5xbt2DhoExx4LtYpqXZnI1kXRPC2twDezZiGE\nz82sHTAc6A0cBNwBHBpC+GIb2+Yk8Feu9Ls1rVypUCgmGzbACy/AkCGwZg1cey2cfLL38xEpZnEG\n/higKbAWuCyEUGpmZUA9YGPYTwwhXLiFbXMS+NOnwymnwPvvR74riUEI8PrrHvyffurj/j//uS/x\nFClGsQV+jXaQo8B//nm/wOellyLflcRszBi/FeMHH3ib5vPOgwYN4q5KJLvUD38btEInOQ49FEaM\ngGefhTfe8H49t90GK1bEXZlIflPgS8Hq2dPH90eMgKlTPfgHD/a7nonI9ynwpeDtvz888YTfZH3B\nAu/Jf9VVPtYvIpUU+FI09twTHnwQpkyBb7/1u3BdfDHMnx93ZSL5oSgCf9UqP5vT+nsB/zm46y5f\nsdWgAXTrBuee6/17RJKsKAJ/3jzvxlinyuuGJUlatvTJ3LIyaNsWDjoIfvYzvwm7SBIVReBrOEe2\npWlTn8ytqICuXb1Vw3HH+R25RJJEgS+Jsf32PplbUeHdOU88EX70I1/XL5IECnxJnAYNfDJ37ly/\nOvvcc6FPH7/5eo7aPonEQoEviVWvnof9rFlw4YXwm9/42v7nnvMePiLFpihaK3ToAK+9BnvtFelu\npMht2OCtOYYMgW++8UZtp5yixQASD/XS2YI1a3xsdsUKNdKS7AgBRo704P/4Y2/UduaZsN12cVcm\nSaJeOlvw0UfeB19hL9li5pO5b74JDz/sQzx77AF33uln/iKFquADX+P3EqU+fXy48IUX/BdA+/Yw\ndCgsXx53ZSKZU+CLpKFHDz/Tf+MNv3CrQwe44Qb4Yqu3/hHJPwp8kQx06gT//CdMnOjtPPbc03vy\nL1oUd2UiVVPgi1RDhw5w//0wbZovHOjUCS66yOeURPKVAl+kBtq29cncDz6AHXaA7t3hnHNgzpy4\nKxP5voIO/HXr/Iyqffu4K5Gka94cbr3VT0B23x0OOQROPRXeey/uykQqFXTgL1jg/9Hq14+7EhHX\npIlP5lZUwAEHwE9+Av37w6RJcVcmUuCBr+EcyVeNG3urhooKD/1TToEjjoDRo9WvR+KjwBeJUP36\n3qenrAxOPx1+9Ss4+GB45RUFv+SeAl8kB+rW9cncWbPg0ku9T0+PHvDMM2rUJrmjwBfJodq1fXhn\n6lS46Sa4/XZf0vnoo7B2bdzVSbFT4IvEwAyOPdYv4LrrLnjoIdh7b/jb32D16rirk2JVsN0yN2yA\nRo1gyRJ/L1Lo3nrLO3ROmwZXXAHnn6+f7SRTt8xNLFzoS+D0H0KKxUEH+WTuSy/B+PF+fcmQIbBs\nWdyVSbEo2MDXcI4Uq+7dfTK3tBRmz/Y2Dtdd53/NitSEAl8kT3Xs6JO5kyd72O+1F1x+OXzySdyV\nSaFS4Ivkufbt4b77YPp0X7u/335wwQUwb17clUmhUeCLFIjWreFPf/Jhnp139huun3WWN24TSYcC\nX6TANGvmk7lz53o//kMPhZNO8rX9IttSkIEfgv+wd+gQdyUi8dlpJ5/MraiAAw+EY46Bfv1gwoS4\nK5N8VZCB/+mnvhxzxx3jrkQkfo0b+2RuebkH/mmnwQ9/6LdjVL8e2VRBBr6Gc0S+r359n8wtK/Ox\n/Ysu8jP/YcMU/OLSCnwzu9TMpqfeLkk99lMzm2Fm682se7Rl/jcFvsjW1a3rgT9zpl+xe9110K0b\nPP00rF8fd3USpyoD38w6AecCBwBdgX5m1h6YDhwPvBlphVugwBepWu3aPpk7ZYpP8v75z96o7eGH\n1agtqdI5w+8ITAohrA4hrAfGACeEEGaHEMqArPZ6SIcCXyR9Zj6hO3483HsvPPaYr+65915YtSru\n6iSX0gn8GUAfM2tiZg2Bo4G20Za1bQp8kcyZweGH+2TuU0/Bq6/6Src77oCvv467OsmFKgM/hPAB\n8HtgJPAqMAWIbSRw45JMBb5I9fXuDS+/7KE/ebJfzfu738FXX8VdmUSpTjpPCiE8BDwEYGZDgAWZ\n7GTw4MHffVxSUkJJSUkmm/+XJUt8bLJp02p/CxFJ6dIF/vUvv1p36FA/kTr/fLjsMr/AS3KntLSU\n0tLSSPeRVj98M2sWQvjczNoBw4HeIYTlqa+NBn4TQnhnK9tmtR/+hAl+i7jJk7P2LUUk5cMP4bbb\nfMjnzDP9Ruxt2sRdVTLF2Q//WTObAbwIXBhCWG5mx5nZAqA3MMzMXstmYVuj4RyR6Oy2G9xzjy/p\nrFMHOnf2M/7y8rgrk2xIK/BDCIeGEPYLIXQLIZSmHnshhNA2hNAghNAqhHBUpJWmKPBFoteqFfzh\nDzBnDrRoAb16wRlnwPvvx12Z1ETBXWmrwBfJnV128cnc8nJfw3/44XDiifDuu3FXJtWhwBeRKu24\nIwwc6I3a+vSB/v3hqKNg3Li4K5NMFNxNzHfeGWbNgubNs/YtRSRDq1fDI4/4yp527WDQIDjiCF/r\nL9kRxaRtQQX+0qU+qbRsmX6wRPLBunW+oueWW7xr56BBcOyxUKvgxg7yT5yrdPJCebkP5yjsRfJD\nnTo+mTtjBlx9Ndx0k6/tf+opNWrLRwUV+Bq/F8lPtWr5ZO477/g6/rvv9puw/+MfsGZN3NXJRgp8\nEckaM5/MHTsWHngAnnzSG7XdfTd8+23c1YkCX0SyzgwOOwxGjoT/+z9/37493H47rFgRd3XJpcAX\nkUj94Afw4ovw+uu+fr99ex/rX7o07sqSR4EvIjnRubMP8YwfDx995EM9V18NixfHXVlyFEzgL1/u\nPbtbtYq7EhGpib328sncd9+FlSt9cveSS2BBRj14pToKJvDLy/1mDVqSKVIcdt3VJ3Pff99vwN6l\nC5x3nv8lL9EomMDXcI5IcWrZ0pdylpVB69Zw4IFw+um+tl+yS4EvInlh5519Mre83Mf7jzgCjj8e\n/vOfuCsrHgp8EckrO+zgk7kVFd6d8/jj4cc/hjFj4q6s8CnwRSQvNWzok7nl5XDSSfCLX8Chh/ry\nzohbgBWtgmme1rq1396wXbssFCUiBWfdOnj6aW/UVr++N2obMKB4G7UltlvmypV+I4aVK4v3H1dE\n0rNhA7z0EgwZ4u0arr0WTj7ZG7kVk8R2y6yogN13V9iLiOfAccfB5Mlwxx1w332wzz7w97+rUVtV\nCiJCNX4vIpszq5zMfegheOYZv1bnL3+Bb76Ju7r8pMAXkYLXpw8MHw7PPw+jR3u/nqFD/Qp9qaTA\nF5GiccABHvqjRsH06R78N94IX3wRd2X5QYEvIkVnv/3g8cdh4kT45BNv1HbllfDpp3FXFi8FvogU\nrT328BuxTJvmN17fd1+46CLv1plEeR/4q1b5b2WtvxeR6mrb1idzZ82C7beH7t39Qq45c+KuLLfy\nPvDnzfOuesW2xlZEcq9FC5/MnTsXdtsNDj4YTj0V3nsv7spyI+8DX8M5IpJtTZrADTf4NT49evjy\nzv79YdKkuCuLlgJfRBJr++19MreiwkP/5JPhyCOhtLQ4+/Uo8EUk8Ro08MncsjI47TQ4/3w45BB4\n9dXiCn4FvohISr16Ppk7axZcfDFcc40P+Tz7rPfwKXR53zytQwd47TW/D6aISC5t2ADDhsHNN/s9\ntQcO9L8AcrGIJHHdMtes8TG2FSv8N6+ISBxC8Kt3hwyB+fP9zP+ss2C77aLbZ+K6ZX70kffBV9iL\nSJzMKidzH3sMXnjBRx/+/Gdv214o8jrwNX4vIvnm4IN9MvfFF2HsWO/Xc8stsGxZ3JVVTYEvIlIN\nGydzR4/2Sd4OHeD662HJkrgr2zoFvohIDey7rw/zTJ4Mn33mC0yuuMKbtuWbtALfzC41s+mpt0tS\njzUxsxFmNtvMXjezHbNdnAJfRApF+/bwt795m4b1671j569/DR9+GHdllaoMfDPrBJwLHAB0BfqZ\nWQfgGmBUCGFv4N/AwGwXp8AXkULTpo1P5n7wgbdw6NEDzj4bZs+Ou7L0zvA7ApNCCKtDCOuBMcAJ\nQH/gkdRzHgGOy2Zh69b5Kp327bP5XUVEcqN5c5/MnTvXx/f79PHWDVOnxldTOoE/A+iTGsJpCBwN\ntAVahBAWA4QQPgWaZ7OwBQv8gNWvn83vKiKSW02a+GRuRQX06gVHHw39+sGECbmvpcrADyF8APwe\nGAm8CkwB1m/pqdksTMM5IlJMGjf2ydyKCjjmGL9i94c/hH//O3f9etK6QDiE8BDwEICZDQEWAIvN\nrEUIYbGZtQQ+29r2gwcP/u7jkpISSkpKqtynAl9EilH9+j6Ze9558MQTcOGF/ldA//6lrFpVimX1\n2tr/llZrBTNrFkL43MzaAcOB3sAgYGkI4fdmdjXQJIRwzRa2rVZrhSuu8JsVXHVVxpuKiBSM9et9\nPf8tt/jn114LJ54IderE1EvHzMYATYG1wGUhhFIzawo8jY/nfwScHEL4agvbVivwBwzwXhUnnJDx\npiIiBScEeOUV79fz5Zcwe3aCmqd16gRPPgmdO0dQlIhIngrBr97t2zchgb9hAzRq5JcoN2oUUWEi\nInksMd0yFy70SQyFvYhI9uRl4JeXa4WOiEi25WXga0mmiEj2KfBFRBJCgS8ikhAKfBGRhMi7ZZkh\n+I3LFy6EHbPeYV9EpDAkYlnm4sXQsKHCXkQk2/Iu8DWcIyISDQW+iEhCKPBFRBJCgS8ikhAKfBGR\nhMirwA9BgS8iEpW8CvwvvoBataBp07grEREpPnkV+Dq7FxGJjgJfRCQhFPgiIgmhwBcRSQgFvohI\nQijwRUQSIm8C/8svYc0aaNYs7kpERIpT3gT+xhuXW1a7P4uIyEZ5E/gazhERiZYCX0QkIRT4IiIJ\nocAXEUkIBb6ISELkReCvWOFvrVrFXYmISPHKi8AvL4cOHbQkU0QkSnkR+BrOERGJXl4E/pw5CnwR\nkajlReBPngw9esRdhYhIcbMQQrQ7MAvb2kcI3j9n6lRo0ybSUkRECoaZEULI6sxm7Gf4s2fD9tsr\n7EVEopZW4JvZQDObaWbvmdnjZlbPzLqY2VtmNs3MXjSzxtUpYNw4OOSQ6mwpIiKZqDLwzWxX4JdA\ntxBCZ6AOcBrwAHBVCKEL8DxwVXUKUOCLiORGOmf4y4E1QCMzqwM0ABYCe4YQxqWeMwo4sToFjBsH\nffpUZ0sREclElYEfQvgSuAOYjwf9shDCKGCmmfVPPe1kIONR+EWL/MYn++yT6ZYiIpKpOlU9wcza\nA5cBuwLLgGfM7GfAL4C7zOx64CX8r4AtGjx48Hcfl5SUUFJSAsD48XDwwVAr9qljEZF4lZaWUlpa\nGuk+qlyWaWYnA0eGEH6Z+vznQK8Qwv9u8pw9gcdCCL23sP1Wl2Veeim0bg1XVWv0X0SkeMW1LHM2\n0NvM6puZAX2BWWbWLFVULeA64L5Md64JWxGR3ElnDH8a8CjwDjANMOB+4DQzmw28DywMITycyY5X\nrPA1+LrCVkQkN2K70nbkSPjd72DMmEh3LyJSkIrqSlsN54iI5JYCX0QkIWIZ0lm7Fpo2hfnzoUmT\nSHcvIlKQimZIZ+pU2H13hb2ISC7FEvgazhERyb1YAn/sWAW+iEiu5XwMPwRo0QL+8x9o1y7SXYuI\nFKyiGMMvK4MGDRT2IiK5lvPA1/i9iEg8FPgiIgmhwBcRSYicBv7ixfD559CpUy73KiIikKPAX7fO\n348fDwcdpBueiIjEISfRu2KFv9dwjohIfBT4IiIJkZPAX74cvv4aZs6Enj1zsUcREdlczs7wn38e\nDjwQ6tfPxR5FRGRzdXKxky++gJtugr//PRd7ExGRLclJL50f/Siwfj2MGhXprkREikYUvXRyEvgQ\nmDABeveOdFciIkWjYJun9eunsBcRiVtOzvAXLQq0bBnpbkREikrBDulEvQ8RkWJTsEM6IiISPwW+\niEhCKPBFRBJCgS8ikhAKfBGRhFDgi4gkhAJfRCQhFPgiIgmhwBcRSQgFvohIQijwRUQSQoEvIpIQ\naQW+mQ00s5lm9p6ZPW5m9czsB2Y22cympN4fEHWxIiJSfVUGvpntCvwS6BZC6IzfFvE04PfAdSGE\nbsCNwO1RFloMSktL4y4hb+hYVNKxqKRjEa10zvCXA2uARmZWB2gILAQWATulnrNT6jHZBv0wV9Kx\nqKRjUUnHIlpV3sQ8hPClmd0BzAe+AUaEEEaZ2RxgvJn9ATDgoGhLFRGRmkhnSKc9cBmwK/A/+Jn+\n6cCDwMUhhHapr/8jykJFRKRmqrzjlZmdDBwZQvhl6vOfA72BM0IIO27yvGWbfr7J47rdlYhINWT7\njldVDukAs4Hrzaw+sBroC7wNzDWzw0IIb5pZX2DOljbOdsEiIlI96YzhTzOzR4F3gPXAFOB+YBLw\nVzOrB6wCzo+yUBERqZnIb2IuIiL5IaMrbc3sJ2b2gZnNMbOrt/Kcv5hZmZlNNbOuVW1rZk3MbISZ\nzTaz183se/MA+SiiY3Gbmc1KPf9ZM9shF6+lpqI4Fpt8/Qoz22BmTaN8DdkS1bEws4tTPxvTzWxo\n1K8jGyL6P9KzEC/4rMax6LbJ4w+a2WIze2+z52eenSGEtN7wXw5z8dU6dYGpwD6bPeco4JXUx72A\niVVti1/AdVXq46uBoenWFNdbhMfiCKBW6uOhwK1xv9a4jkXq622A4cA8oGncrzXGn4sSYARQJ/X5\nLnG/1hiPxWjgR5tsPzru1xrlsUh9fgjQFXhvs20yzs5MzvB/AJSFED4KIawFngIGbPacAcCjACGE\nScCOZtaiim0HAI+kPn4EOC6DmuISybEIIYwKIWxIbT8RD7x8F9XPBcCfgCujfgFZFNWx+DX+n3ld\narsl0b+UGovqWCwCNp7JFsoFnzU5FoQQxgFfbuH7ZpydmQR+a2DBJp9/nHosnedsa9sWIYTFACGE\nT4HmGdQUl6iOxaZ+AbxW40qjF8mxMLP+wIIQwvRsFxyhqH4u9gIONbOJZja6QIYxojoW1wB/NLP5\nwG3AwCzWHJXqHIuFW3jO5ppnmp1Rd8uszpLMYp1FTvtYmNkgYG0I4YkI64nTNo+FmTUArsV7NKW1\nTQFL53XVAZqEEHoDVwFPR1tSbNI5Frrgc+uqzM5MAn8h0G6Tz9vw/T+nFgJtt/CcbW376cY/Xcys\nJfBZBjXFJapjgZmdDRwN/Cx75UYqimPRAdgNmGZm81KPv2Nm+f7XX1Q/Fx8DzwGEEN4GNpjZztkr\nOxJRHYteIYQXAEIIz+DDJfmuJsdiWxZnnJ0ZTDzUpnLioR4+8dBxs+ccTeXEQ28qJ2G2ui0+8XB1\nJhMPcb9FeCx+AswEdo77NcZ9LDbbfh5+hhv7643p5+JXwE2pj/cCPor7tcZwLDZO2r4DHJb6uC/w\ndtyvNcpjscnXdwOmb/ZYxtmZaeE/wa+8LQOu2eSH8fxNnnN36sVNA7pva9vU402BUamvjQB2ivsf\nKMZjUQZ8BLybersn7tcZ17HY7PtXUACrdCL8uagLPAZMB/6zMfDy/S2iY3EAftHnFGAC3rY99tca\n8bF4AvgE73QwHzgn9XjG2akLr0REEkK3OBQRSQgFvohIQijwRUQSQoEvIpIQCnwRkYRQ4IuIJIQC\nX0QkIRT4IiIJ8f/Q9e24keOCdQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc8341ab908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(regularization_parameters, acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extreme Overfitting Case: Neural Network\n",
    "Restrict training data to just a few. No SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_subset_size = 100\n",
    "hidden_layer_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \"\"\"Input data. For the training data, we use a placeholder that will be fed\n",
    "    at run time with a training minibatch\"\"\"\n",
    "    tf_train_dataset = tf.constant(train_dataset[:train_subset_size])\n",
    "    tf_train_labels = tf.constant(train_labels[:train_subset_size])\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    \"\"\"    Variables - Weights\n",
    "    \n",
    "    These are the parameters that we are going to be training.\n",
    "    Weights are initiallized randomly and the biases to zero\n",
    "    \"\"\"\n",
    "    weights1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_layer_nodes]))\n",
    "    biases1 = tf.Variable(tf.zeros([hidden_layer_nodes]))\n",
    "    \n",
    "    weights2 = tf.Variable(tf.truncated_normal([hidden_layer_nodes, num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    \"\"\"Training Computation\n",
    "  \n",
    "    We multiply the inputs with the weight matrix, and add biases. \n",
    "    We compute then calculate the softmax and cross-entropy (it's one \n",
    "    operation in TensorFlow, because it's very common, and it can be optimized). \n",
    "    We take the average of this cross-entropy across all training examples: that's our loss.\n",
    "    \"\"\"\n",
    "    hidden = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    logits = tf.matmul(hidden, weights2) + biases2\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "    \"\"\"Optimizer\n",
    "    We now find the minimum of this loss using gradient descent\n",
    "    \"\"\"\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    \"\"\"Predictions for the training, validation and test data.\n",
    "    \n",
    "    These are not part of training, but merely here so that we can report\n",
    "    accuracy figures as we train\n",
    "    \"\"\"\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    hidden_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(hidden_valid, weights2) + biases2)\n",
    "    \n",
    "    hidden_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(hidden_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 355.050690\n",
      "Training Accuracy: \t5.0%\n",
      "Validation Accuracy: \t20.0%\n",
      "Loss at step 100: 0.000000\n",
      "Training Accuracy: \t100.0%\n",
      "Validation Accuracy: \t53.1%\n",
      "Loss at step 200: 0.000000\n",
      "Training Accuracy: \t100.0%\n",
      "Validation Accuracy: \t53.1%\n",
      "Loss at step 300: 0.000000\n",
      "Training Accuracy: \t100.0%\n",
      "Validation Accuracy: \t53.1%\n",
      "\n",
      "Test Accuracy: \t\t56.8%\n",
      "CPU times: user 22.7 s, sys: 1.6 s, total: 24.3 s\n",
      "Wall time: 8.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Running the computation\n",
    "num_steps = 301\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    return 100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0]\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    \"\"\"  This is a one-time operation which ensures the parameters get initialized as\n",
    "    we described in the graph: random weights for the matrix, zeros for the\n",
    "    biases.\"\"\"\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        \"\"\"Run the computations. We tell .run() that we want to run the optimizer,\n",
    "        and get the loss value and the training predictions returned as numpy\n",
    "        arrays.\"\"\"\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "        if step % 100 == 0:\n",
    "            print(\"Loss at step %d: %f\" %(step, l))\n",
    "            print(\"Training Accuracy: \\t%.1f%%\" % accuracy(predictions, train_labels[:train_subset_size]))\n",
    "            \"\"\"Calling .eval() on valid_prediction is basically like calling run(), but\n",
    "            just to get that one numpy array. Note that it recomputes all its graph\n",
    "            dependencies.\"\"\"\n",
    "            print(\"Validation Accuracy: \\t%.1f%%\" %accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"\\nTest Accuracy: \\t\\t%.1f%%\" %accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout Technique on Hidden Layer\n",
    "Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides nn.dropout() for that, but you have to make sure it's only inserted during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_subset_size = 100\n",
    "hidden_layer_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \"\"\"Input data. For the training data, we use a placeholder that will be fed\n",
    "    at run time with a training minibatch\"\"\"\n",
    "    tf_train_dataset = tf.constant(train_dataset[:train_subset_size])\n",
    "    tf_train_labels = tf.constant(train_labels[:train_subset_size])\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    \"\"\"    Variables - Weights\n",
    "    \n",
    "    These are the parameters that we are going to be training.\n",
    "    Weights are initiallized randomly and the biases to zero\n",
    "    \"\"\"\n",
    "    weights1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_layer_nodes]))\n",
    "    biases1 = tf.Variable(tf.zeros([hidden_layer_nodes]))\n",
    "    \n",
    "    weights2 = tf.Variable(tf.truncated_normal([hidden_layer_nodes, num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    \n",
    "    \"\"\"Training Computation\n",
    "  \n",
    "    We multiply the inputs with the weight matrix, and add biases. \n",
    "    We compute then calculate the softmax and cross-entropy (it's one \n",
    "    operation in TensorFlow, because it's very common, and it can be optimized). \n",
    "    We take the average of this cross-entropy across all training examples: that's our loss.\n",
    "    \"\"\"\n",
    "    hidden = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    hidden_drop = tf.nn.dropout(hidden, keep_prob=keep_prob)\n",
    "    logits = tf.matmul(hidden_drop, weights2) + biases2\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "    \"\"\"Optimizer\n",
    "    We now find the minimum of this loss using gradient descent\n",
    "    \"\"\"\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    \"\"\"Predictions for the training, validation and test data.\n",
    "    \n",
    "    These are not part of training, but merely here so that we can report\n",
    "    accuracy figures as we train\n",
    "    \"\"\"\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    hidden_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(hidden_valid, weights2) + biases2)\n",
    "    \n",
    "    hidden_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(hidden_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 407.833954\n",
      "Accuracy on training with dropout: \t13.0%\n",
      "Training Accuracy: \t30.0%\n",
      "Validation Accuracy: \t16.6%\n",
      "Loss at step 100: 0.000000\n",
      "Accuracy on training with dropout: \t100.0%\n",
      "Training Accuracy: \t100.0%\n",
      "Validation Accuracy: \t60.5%\n",
      "Loss at step 200: 0.000000\n",
      "Accuracy on training with dropout: \t100.0%\n",
      "Training Accuracy: \t100.0%\n",
      "Validation Accuracy: \t60.9%\n",
      "Loss at step 300: 0.000000\n",
      "Accuracy on training with dropout: \t100.0%\n",
      "Training Accuracy: \t100.0%\n",
      "Validation Accuracy: \t60.9%\n",
      "\n",
      "Test Accuracy: \t\t65.4%\n",
      "CPU times: user 23.6 s, sys: 1.65 s, total: 25.2 s\n",
      "Wall time: 8.63 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Running the computation\n",
    "num_steps = 301\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    return 100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0]\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    \"\"\"  This is a one-time operation which ensures the parameters get initialized as\n",
    "    we described in the graph: random weights for the matrix, zeros for the\n",
    "    biases.\"\"\"\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        \"\"\"Run the computations. We tell .run() that we want to run the optimizer,\n",
    "        and get the loss value and the training predictions returned as numpy\n",
    "        arrays.\"\"\"\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict={keep_prob : 0.6})\n",
    "        if step % 100 == 0:\n",
    "            print(\"Loss at step %d: %f\" %(step, l))\n",
    "            print(\"Accuracy on training with dropout: \\t%.1f%%\" %accuracy(predictions, train_labels[:train_subset_size]))\n",
    "            print(\"Training Accuracy: \\t%.1f%%\" %accuracy(train_prediction.eval({keep_prob : 1.0}), train_labels[:train_subset_size]))\n",
    "            \"\"\"Calling .eval() on valid_prediction is basically like calling run(), but\n",
    "            just to get that one numpy array. Note that it recomputes all its graph\n",
    "            dependencies.\"\"\"\n",
    "            print(\"Validation Accuracy: \\t%.1f%%\" %accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"\\nTest Accuracy: \\t\\t%.1f%%\" %accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Network + SGD - 1 Layer - Dropout\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is 97.1%.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "`python\n",
    "global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "hidden_layer_1_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \"\"\"Input data. For the training data, we use a placeholder that will be fed\n",
    "    at run time with a training minibatch\"\"\"\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape = (batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape = (batch_size, num_labels))\n",
    "    \n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    \"\"\"    Variables - Weights\n",
    "    \n",
    "    These are the parameters that we are going to be training.\n",
    "    Weights are initiallized randomly and the biases to zero\n",
    "    \"\"\"\n",
    "    weights1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_layer_1_nodes]))\n",
    "    biases1 = tf.Variable(tf.zeros([hidden_layer_1_nodes]))\n",
    "    \n",
    "    weights2 = tf.Variable(tf.truncated_normal([hidden_layer_1_nodes, num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    \"\"\"Training Computation\n",
    "  \n",
    "    We multiply the inputs with the weight matrix, and add biases. \n",
    "    We compute then calculate the softmax and cross-entropy (it's one \n",
    "    operation in TensorFlow, because it's very common, and it can be optimized). \n",
    "    We take the average of this cross-entropy across all training examples: that's our loss.\n",
    "    \"\"\"\n",
    "    hidden1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    hidden1 = tf.nn.dropout(hidden1, 0.6)\n",
    "    logits = tf.matmul(hidden1, weights2) + biases2\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) \n",
    "#            tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2)\n",
    "    \n",
    "    \"\"\"Optimizer\n",
    "    We now find the minimum of this loss using gradient descent\n",
    "    \"\"\"\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, 100000, 0.95, staircase = True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "#     optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    \"\"\"Predictions for the training, validation and test data.\n",
    "    \n",
    "    These are not part of training, but merely here so that we can report\n",
    "    accuracy figures as we train\n",
    "    \"\"\"\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    hidden1 = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(hidden1, weights2) + biases2)\n",
    "    \n",
    "    hidden1 = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(hidden1, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 447.419128\n",
      "Minibatch accuracy: \t6.2%\n",
      "Validation accuracy: \t21.8%\n",
      "Minibatch loss at step 500: 40.777122\n",
      "Minibatch accuracy: \t70.3%\n",
      "Validation accuracy: \t79.4%\n",
      "Minibatch loss at step 1000: 23.834034\n",
      "Minibatch accuracy: \t75.8%\n",
      "Validation accuracy: \t77.9%\n",
      "Minibatch loss at step 1500: 12.013689\n",
      "Minibatch accuracy: \t70.3%\n",
      "Validation accuracy: \t79.6%\n",
      "Minibatch loss at step 2000: 9.719074\n",
      "Minibatch accuracy: \t69.5%\n",
      "Validation accuracy: \t80.5%\n",
      "Minibatch loss at step 2500: 2.278395\n",
      "Minibatch accuracy: \t77.3%\n",
      "Validation accuracy: \t80.0%\n",
      "Minibatch loss at step 3000: 2.928926\n",
      "Minibatch accuracy: \t78.1%\n",
      "Validation accuracy: \t79.9%\n",
      "Minibatch loss at step 3500: 4.783579\n",
      "Minibatch accuracy: \t78.1%\n",
      "Validation accuracy: \t79.8%\n",
      "\n",
      "Test accuracy: 86.4%\n",
      "CPU times: user 4min 16s, sys: 15.9 s, total: 4min 32s\n",
      "Wall time: 1min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        \n",
    "        \"\"\"This offset is an integer of the batch size, Such that \n",
    "        we are taking the batches not in the order they are presented.\"\"\"\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        \n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size)]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size)]\n",
    "        \n",
    "        \"\"\"Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        and the value is the numpy array to feed to it.\"\"\"\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        \n",
    "        # Perform calculations\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict = feed_dict)\n",
    "        \n",
    "        # Give feedback every 500 steps\n",
    "        if step % 500 == 0:\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: \\t%.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: \\t%.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"\\nTest accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Network + SGD - 2 Layer - Dropout\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is 97.1%.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "`python\n",
    "global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "hidden_layer_1_nodes = 1024\n",
    "hidden_layer_2_nodes = 512\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \"\"\"Input data. For the training data, we use a placeholder that will be fed\n",
    "    at run time with a training minibatch\"\"\"\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape = (batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape = (batch_size, num_labels))\n",
    "    \n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    \"\"\"    Variables - Weights\n",
    "    \n",
    "    These are the parameters that we are going to be training.\n",
    "    Weights are initiallized randomly and the biases to zero\n",
    "    \"\"\"\n",
    "    weights1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_layer_1_nodes]))\n",
    "    biases1 = tf.Variable(tf.zeros([hidden_layer_1_nodes]))\n",
    "    \n",
    "    weights2 = tf.Variable(tf.truncated_normal([hidden_layer_1_nodes, hidden_layer_2_nodes]))\n",
    "    biases2 = tf.Variable(tf.zeros([hidden_layer_2_nodes]))\n",
    "    \n",
    "    weights3 = tf.Variable(tf.truncated_normal([hidden_layer_2_nodes, num_labels]))\n",
    "    biases3 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    \"\"\"Training Computation\n",
    "  \n",
    "    We multiply the inputs with the weight matrix, and add biases. \n",
    "    We compute then calculate the softmax and cross-entropy (it's one \n",
    "    operation in TensorFlow, because it's very common, and it can be optimized). \n",
    "    We take the average of this cross-entropy across all training examples: that's our loss.\n",
    "    \"\"\"\n",
    "    hidden1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "#     hidden1 = tf.nn.dropout(hidden1, 0.6)\n",
    "    hidden2 = tf.nn.relu(tf.matmul(hidden1, weights2) + biases2)\n",
    "#     hidden2 = tf.nn.dropout(hidden2, 0.6)\n",
    "    logits = tf.matmul(hidden2, weights3) + biases3\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "    \"\"\"Optimizer\n",
    "    We now find the minimum of this loss using gradient descent\n",
    "    \"\"\"\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.001, global_step, 100000, 0.95, staircase = True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "#     optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    \"\"\"Predictions for the training, validation and test data.\n",
    "    \n",
    "    These are not part of training, but merely here so that we can report\n",
    "    accuracy figures as we train\n",
    "    \"\"\"\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    hidden1 = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "    hidden2 = tf.nn.relu(tf.matmul(hidden1, weights2) + biases2)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(hidden2, weights3) + biases3)\n",
    "    \n",
    "    hidden1 = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    hidden2 = tf.nn.relu(tf.matmul(hidden1, weights2) + biases2)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(hidden2, weights3) + biases3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3781.179443\n",
      "Minibatch accuracy: \t12.9%\n",
      "Validation accuracy: \t21.7%\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_steps = 30001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        \n",
    "        \"\"\"This offset is an integer of the batch size, Such that \n",
    "        we are taking the batches not in the order they are presented.\"\"\"\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        \n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size)]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size)]\n",
    "        \n",
    "        \"\"\"Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        and the value is the numpy array to feed to it.\"\"\"\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        \n",
    "        # Perform calculations\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict = feed_dict)\n",
    "        \n",
    "        # Give feedback every 500 steps\n",
    "        if step % 2000 == 0:\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: \\t%.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: \\t%.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"\\nTest accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Network + SGD - 3 Layer - Dropout\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is 97.1%.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "`python\n",
    "global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "hidden_layer_1_nodes = 1024\n",
    "hidden_layer_2_nodes = 512\n",
    "hidden_layer_3_nodes = 256\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \"\"\"Input data. For the training data, we use a placeholder that will be fed\n",
    "    at run time with a training minibatch\"\"\"\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape = (batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape = (batch_size, num_labels))\n",
    "    \n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    \"\"\"    Variables - Weights\n",
    "    \n",
    "    These are the parameters that we are going to be training.\n",
    "    Weights are initiallized randomly and the biases to zero\n",
    "    \"\"\"\n",
    "    weights1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_layer_1_nodes]))\n",
    "    biases1 = tf.Variable(tf.zeros([hidden_layer_1_nodes]))\n",
    "    \n",
    "    weights2 = tf.Variable(tf.truncated_normal([hidden_layer_1_nodes, hidden_layer_2_nodes]))\n",
    "    biases2 = tf.Variable(tf.zeros([hidden_layer_2_nodes]))\n",
    "    \n",
    "    weights3 = tf.Variable(tf.truncated_normal([hidden_layer_2_nodes, hidden_layer_3_nodes]))\n",
    "    biases3 = tf.Variable(tf.zeros([hidden_layer_3_nodes]))\n",
    "    \n",
    "    weights4 = tf.Variable(tf.truncated_normal([hidden_layer_3_nodes, num_labels]))\n",
    "    biases4 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    \"\"\"Training Computation\n",
    "  \n",
    "    We multiply the inputs with the weight matrix, and add biases. \n",
    "    We compute then calculate the softmax and cross-entropy (it's one \n",
    "    operation in TensorFlow, because it's very common, and it can be optimized). \n",
    "    We take the average of this cross-entropy across all training examples: that's our loss.\n",
    "    \"\"\"\n",
    "    hidden1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    hidden1 = tf.nn.dropout(hidden1, 0.6)\n",
    "    hidden2 = tf.nn.relu(tf.matmul(hidden1, weights2) + biases2)\n",
    "    hidden2 = tf.nn.dropout(hidden2, 0.6)\n",
    "    hidden3 = tf.nn.relu(tf.matmul(hidden2, weights3) + biases3)\n",
    "    hidden3 = tf.nn.dropout(hidden3, 0.6)\n",
    "    logits = tf.matmul(hidden3, weights4) + biases4\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "#            .001 * tf.nn.l2_loss(weights1) + .001 * tf.nn.l2_loss(weights2) + .001 * tf.nn.l2_loss(weights3) \n",
    "    \n",
    "    \n",
    "    \"\"\"Optimizer\n",
    "    We now find the minimum of this loss using gradient descent\n",
    "    \"\"\"\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.00001, global_step, 100000, 0.95, staircase = True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "#     optimizer = tf.train.GradientDescentOptimizer(0.0001).minimize(loss)\n",
    "    \n",
    "    \"\"\"Predictions for the training, validation and test data.\n",
    "    \n",
    "    These are not part of training, but merely here so that we can report\n",
    "    accuracy figures as we train\n",
    "    \"\"\"\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    hidden1 = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "    hidden2 = tf.nn.relu(tf.matmul(hidden1, weights2) + biases2)\n",
    "    hidden3 = tf.nn.relu(tf.matmul(hidden2, weights3) + biases3)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(hidden3, weights4) + biases4)\n",
    "    \n",
    "    hidden1 = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    hidden2 = tf.nn.relu(tf.matmul(hidden1, weights2) + biases2)\n",
    "    hidden3 = tf.nn.relu(tf.matmul(hidden2, weights3) + biases3)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(hidden3, weights4) + biases4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 103468.234375\n",
      "Minibatch accuracy: \t14.1%\n",
      "Validation accuracy: \t9.4%\n",
      "Minibatch loss at step 1000: 16517.367188\n",
      "Minibatch accuracy: \t41.0%\n",
      "Validation accuracy: \t69.3%\n",
      "Minibatch loss at step 2000: 10110.689453\n",
      "Minibatch accuracy: \t42.2%\n",
      "Validation accuracy: \t71.6%\n",
      "Minibatch loss at step 3000: 6874.708496\n",
      "Minibatch accuracy: \t44.5%\n",
      "Validation accuracy: \t71.3%\n",
      "Minibatch loss at step 4000: 4535.848633\n",
      "Minibatch accuracy: \t48.0%\n",
      "Validation accuracy: \t71.5%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-253-ba5fb5909d61>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'num_steps = 30001\\nbatch_size = 256\\n\\nwith tf.Session(graph=graph) as session:\\n    tf.initialize_all_variables().run()\\n    print(\"Initialized\")\\n    for step in range(num_steps):\\n        \\n        \"\"\"This offset is an integer of the batch size, Such that \\n        we are taking the batches not in the order they are presented.\"\"\"\\n        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\\n        \\n        # Generate a minibatch.\\n        batch_data = train_dataset[offset:(offset + batch_size)]\\n        batch_labels = train_labels[offset:(offset + batch_size)]\\n        \\n        \"\"\"Prepare a dictionary telling the session where to feed the minibatch.\\n        The key of the dictionary is the placeholder node of the graph to be fed,\\n#         and the value is the numpy array to feed to it.\"\"\"\\n        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\\n        \\n        # Perform calculations\\n        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict = feed_dict)\\n        \\n        # Give feedback every 500 steps\\n        if step % 1000 == 0:\\n            print(\"Minibatch loss at step %d: %f\" % (step, l))\\n            print(\"Minibatch accuracy: \\\\t%.1f%%\" % accuracy(predictions, batch_labels))\\n            print(\"Validation accuracy: \\\\t%.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\\n    print(\"\\\\nTest accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2118\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2119\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2120\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2121\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1175\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1176\u001b[0m             \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1177\u001b[1;33m             \u001b[0mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1178\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    338\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 340\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    341\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    562\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m       results = self._do_run(handle, target_list, unique_fetches,\n\u001b[1;32m--> 564\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    565\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    566\u001b[0m       \u001b[1;31m# The movers are no longer used. Delete them.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    635\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m--> 637\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m    638\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    639\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m    642\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    643\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 644\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    645\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatusNotOK\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    646\u001b[0m       \u001b[0merror_message\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_message\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m    626\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m--> 628\u001b[1;33m             session, None, feed_dict, fetch_list, target_list, None)\n\u001b[0m\u001b[0;32m    629\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    630\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_steps = 30001\n",
    "batch_size = 256\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        \n",
    "        \"\"\"This offset is an integer of the batch size, Such that \n",
    "        we are taking the batches not in the order they are presented.\"\"\"\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        \n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size)]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size)]\n",
    "        \n",
    "        \"\"\"Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "#         and the value is the numpy array to feed to it.\"\"\"\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        \n",
    "        # Perform calculations\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict = feed_dict)\n",
    "        \n",
    "        # Give feedback every 500 steps\n",
    "        if step % 1000 == 0:\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: \\t%.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: \\t%.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"\\nTest accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
